{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data_batch_5', 'batches.meta', 'data_batch_4', 'data_batch_3', 'data_batch_1', 'readme.html', 'data_batch_2', 'test_batch']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle as p\n",
    "import numpy as np \n",
    "\n",
    "CIFAR_DIR = \"./cifar-10-batches-py\"\n",
    "print(os.listdir(CIFAR_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard\n",
    "# 1. 指定面板图上显示的变量\n",
    "# 2. 训练过程中将这些变量计算出来，输出到文件中\n",
    "# 3. 文件解析 ./ensorboard --logdir=dir.\n",
    "\n",
    "# batch normalization: 需要在每个卷积后添加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n",
      "(50000,)\n",
      "(10000, 3072)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"read data from data file\"\"\"\n",
    "    with open(filename,'rb') as f:\n",
    "        data = p.load(f,encoding='bytes')\n",
    "        return data[b'data'],data[b'labels']\n",
    "    \n",
    "class CifarData:\n",
    "    def __init__(self,filenames,need_shuffle):\n",
    "        all_data = []\n",
    "        all_labels = []\n",
    "        for filename in filenames:\n",
    "            data, labels = load_data(filename)\n",
    "            all_data.append(data)\n",
    "            all_labels.append(labels)\n",
    "        self._data = np.vstack(all_data)  #纵向合并成一个矩阵\n",
    "        self._labels = np.hstack(all_labels)  #横向合并～\n",
    "        print(self._data.shape)\n",
    "        print(self._labels.shape)\n",
    "  \n",
    "        self._num_examples = self._data.shape[0]  #样本数目\n",
    "        self._need_shuffle = need_shuffle   \n",
    "        #样本数据是否需要打乱，是否可以重复使用数据样本\n",
    "        self._indicator = 0  #表明当前数据集已经定位到哪个位置了\n",
    "        if self._need_shuffle:\n",
    "            self._shuffle_data()\n",
    "            \n",
    "    def _shuffle_data(self):\n",
    "        #顺序打乱  将0～(num-1)的数混乱排序并输出\n",
    "        p = np.random.permutation(self._num_examples)\n",
    "        self._data = self._data[p]\n",
    "        self._labels = self._labels[p]\n",
    "        \n",
    "    def next_batch(self,batch_size):\n",
    "        \"\"\"return batch_size example as a batch\"\"\"\n",
    "        end_indicator = self._indicator + batch_size #定义结束位置\n",
    "        if end_indicator > self._num_examples:  \n",
    "            if self._need_shuffle:\n",
    "                self._shuffle_data()\n",
    "                self._indicator = 0\n",
    "                end_indicator = batch_size\n",
    "            else:\n",
    "                raise Exception(\"have no more example\")\n",
    "        if end_indicator > self._num_examples:\n",
    "        #这个判断batch_size是不是大于样本总数\n",
    "            raise Exception(\"batch size is larger than all examples\")\n",
    "        batch_data = self._data[self._indicator:end_indicator]\n",
    "        batch_labels = self._labels[self._indicator:end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_data,batch_labels\n",
    "    \n",
    "train_filenames = [os.path.join(CIFAR_DIR,'data_batch_%d' % i) for i in range(1,6)]\n",
    "test_filenames = [os.path.join(CIFAR_DIR,'test_batch')]\n",
    "\n",
    "train_data = CifarData(train_filenames,True)\n",
    "test_data = CifarData(test_filenames,False)\n",
    "\n",
    "# batch_data, batch_labels = train_data.next_batch(10)\n",
    "# print(batch_data)\n",
    "# print(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size=20\n",
    "x = tf.placeholder(tf.float32,[batch_size,3072])\n",
    "y = tf.placeholder(tf.int64,[batch_size])\n",
    "is_training = tf.placeholder(tf.bool, [])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 3, 32, 32])\n",
    "# 32*32\n",
    "x_image = tf.transpose(x_image, perm=[0, 2, 3, 1])\n",
    "\n",
    "x_image_arr = tf.split(x_image,\n",
    "                       num_or_size_splits = batch_size,\n",
    "                       axis = 0)\n",
    "result_x_image_arr = []\n",
    "for x_single_image in x_image_arr:\n",
    "    # x_single_image: [1, 32, 32, 3] -> [32, 32, 3]\n",
    "    x_single_image = tf.reshape(x_single_image, [32, 32, 3])  \n",
    "    data_aug_1 = tf.image.random_flip_left_right(x_single_image)\n",
    "    data_aug_2 = tf.image.random_brightness(data_aug_1, max_delta=63)\n",
    "    data_aug_3 = tf.image.random_contrast(\n",
    "        data_aug_2, lower=0.2, upper=1.8)\n",
    "    x_single_image = tf.reshape(data_aug_3, [1, 32, 32, 3])\n",
    "    result_x_image_arr.append(x_single_image)\n",
    "result_x_images = tf.concat(result_x_image_arr, axis=0)\n",
    "normal_result_x_images = result_x_images / 127.5 -1\n",
    "\n",
    "# 抽象化卷积\n",
    "'''\n",
    "def conv_wrapper(inputs,\n",
    "                 name,\n",
    "                 output_channel=32,\n",
    "                 kernel_size=(3,3),\n",
    "                 activation = tf.nn.relu,\n",
    "                 padding = 'same'):\n",
    "    \"\"\"wrapper of tf.layers.conv2d\"\"\"\n",
    "    return tf.layers.conv2d(inputs,\n",
    "                            output_channel,\n",
    "                            kernel_size,\n",
    "                            padding = padding,\n",
    "                            activation = activation,\n",
    "                            name = name)\n",
    "'''\n",
    "\n",
    "def conv_wrapper(inputs,\n",
    "                 name,\n",
    "                 is_training,\n",
    "                 output_channel=32,\n",
    "                 kernel_size=(3,3),\n",
    "                 activation = tf.nn.relu,\n",
    "                 padding = 'same'):\n",
    "    \"\"\"wrapper of tf.layers.conv2d\"\"\"\n",
    "    # without bn: conv -> activation\n",
    "    # with batch normalization: conv -> bn -> activation\n",
    "    with tf.name_scope(name):\n",
    "        conv2d = tf.layers.conv2d(inputs,\n",
    "                                  output_channel,\n",
    "                                  kernel_size,\n",
    "                                  padding = padding,\n",
    "                                  activation = None,\n",
    "                                  name = name + '/conv2d')\n",
    "        # bn 在train过程中是用一个batch计算均值和方差，\n",
    "        # 在test过程中用整个数据集上计算得到\n",
    "        bn = tf.layers.batch_normalization(conv2d,\n",
    "                                           training = is_training) # 判断是不是在训练\n",
    "        return activation(bn)\n",
    "# 抽象化池化  （wrapper:封套）\n",
    "def pooling_wrapper(inputs, name):\n",
    "    \"\"\"wrapper of tf.layers.max_pooling2d\"\"\"\n",
    "    return tf.layers.max_pooling2d(inputs,\n",
    "                                   (2,2),\n",
    "                                   (2,2),\n",
    "                                   name = name)\n",
    "\n",
    "#conv1: 神经元图， feature_map，输出图像\n",
    "conv1_1 = conv_wrapper(normal_result_x_images, 'conv1_1', is_training)\n",
    "conv1_2 = conv_wrapper(conv1_1, 'conv1_2', is_training)\n",
    "conv1_3 = conv_wrapper(conv1_2, 'conv1_3', is_training)\n",
    "pooling1 = pooling_wrapper(conv1_3, 'pool1')\n",
    "\n",
    "conv2_1 = conv_wrapper(pooling1, 'conv2_1', is_training)\n",
    "conv2_2 = conv_wrapper(conv2_1, 'conv2_2', is_training)\n",
    "conv2_3 = conv_wrapper(conv2_2, 'conv2_3', is_training)\n",
    "pooling2 = pooling_wrapper(conv2_3, 'pool2')\n",
    "\n",
    "conv3_1 = conv_wrapper(pooling2, 'conv3_1', is_training)\n",
    "conv3_2 = conv_wrapper(conv3_1, 'comv3_2', is_training)\n",
    "conv3_3 = conv_wrapper(conv3_2, 'conv3_3', is_training)\n",
    "pooling3 = pooling_wrapper(conv3_3, 'pool3')\n",
    "\n",
    "# 展平后变为二维向量 [None, 4 * 4 * 32]\n",
    "flatten = tf.layers.flatten(pooling3)\n",
    "y_ = tf.layers.dense(flatten, 10)\n",
    "\n",
    "# 交叉熵损失\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels = y, logits = y_)\n",
    "# y_ -> softmax\n",
    "#y ->one_hot\n",
    "#loss = ylogy_\n",
    "\n",
    "\n",
    "# indices\n",
    "predict = tf.argmax(y_, 1) #在y_的第一维求最大值的索引位置\n",
    "correct_prediction = tf.equal(predict, y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64))\n",
    "\n",
    "with tf.name_scope('train_op'):\n",
    "#     train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    # 有两个方案使用batch_normalization, 第一种如下面的代码使用control_dependencies\n",
    "    # 第二种是不使用control_dependencies,但在下面训练代码中调用sess.run时把\n",
    "    # update_ops也加进去，即 sess.run([train_op, update_ops, ..],feed_dict = ..)\n",
    "    with tf.control_dependencies(update_ops): # 保证train_op在update_ops执行后再执行\n",
    "        train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_summary(var, name):\n",
    "    \"\"\"Constructs summary for statistics of a variable\"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "        \n",
    "with tf.name_scope('summary'):\n",
    "    variable_summary(conv1_1, 'conv1_1')\n",
    "    variable_summary(conv1_2, 'conv1_2')\n",
    "    variable_summary(conv2_1, 'conv2_1')\n",
    "    variable_summary(conv2_2, 'conv2_2')\n",
    "    variable_summary(conv3_1, 'conv3_1')\n",
    "    variable_summary(conv3_2, 'conv3_2')\n",
    "\n",
    "loss_summary = tf.summary.scalar('loss', loss)\n",
    "# 'loss': <10, 1.1>, <20, 1.08>\n",
    "accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "input_summary = tf.summary.image('inputs_image', result_x_images)\n",
    "\n",
    "merged_summary = tf.summary.merge_all()   #把所有summary合并到一起\n",
    "merged_summary_test = tf.summary.merge([loss_summary, accuracy_summary])\n",
    "\n",
    "LOG_DIR = '.'\n",
    "run_label = './run_VGG_Net_tensorboard'\n",
    "run_dir = os.path.join(LOG_DIR, run_label)\n",
    "if not os.path.exists(run_dir):\n",
    "    os.mkdir(run_dir)\n",
    "train_log_dir = os.path.join(run_dir, 'train')\n",
    "test_log_dir = os.path.join(run_dir, 'test')\n",
    "if not os.path.exists(train_log_dir):\n",
    "    os.mkdir(train_log_dir)\n",
    "if not os.path.exists(test_log_dir):\n",
    "    os.mkdir(test_log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dir = os.path.join(run_dir, 'model')\n",
    "# if not os.path.exists(model_dir):\n",
    "#     os.mkdir(model_dir)\n",
    "    \n",
    "# saver = tf.train.Saver(var_list = tf.global_variables())\n",
    "# model_name = 'ckp-01000'\n",
    "# model_path = os.path.join(model_dir, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Step: 500, loss: 1.84517, acc: 0.30000\n",
      "[Train] Step: 1000, loss: 1.32742, acc: 0.50000\n",
      "[Train] Step: 1500, loss: 1.57219, acc: 0.30000\n",
      "[Train] Step: 2000, loss: 1.12057, acc: 0.60000\n",
      "[Train] Step: 2500, loss: 0.74552, acc: 0.80000\n",
      "[Train] Step: 3000, loss: 0.64952, acc: 0.75000\n",
      "[Train] Step: 3500, loss: 1.42837, acc: 0.55000\n",
      "[Train] Step: 4000, loss: 0.73900, acc: 0.70000\n",
      "[Train] Step: 4500, loss: 0.57663, acc: 0.80000\n",
      "[Train] Step: 5000, loss: 1.05474, acc: 0.55000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Step: 5000, acc: 0.66250\n",
      "[Train] Step: 5500, loss: 0.56174, acc: 0.70000\n",
      "[Train] Step: 6000, loss: 0.92993, acc: 0.75000\n",
      "[Train] Step: 6500, loss: 0.50995, acc: 0.85000\n",
      "[Train] Step: 7000, loss: 0.56809, acc: 0.80000\n",
      "[Train] Step: 7500, loss: 0.84469, acc: 0.70000\n",
      "[Train] Step: 8000, loss: 0.67090, acc: 0.75000\n",
      "[Train] Step: 8500, loss: 1.33463, acc: 0.60000\n",
      "[Train] Step: 9000, loss: 0.66930, acc: 0.75000\n",
      "[Train] Step: 9500, loss: 0.67765, acc: 0.80000\n",
      "[Train] Step: 10000, loss: 1.05175, acc: 0.70000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Step: 10000, acc: 0.73800\n",
      "[Train] Step: 10500, loss: 0.52082, acc: 0.80000\n",
      "[Train] Step: 11000, loss: 0.82164, acc: 0.80000\n",
      "[Train] Step: 11500, loss: 0.61486, acc: 0.75000\n",
      "[Train] Step: 12000, loss: 0.44729, acc: 0.90000\n",
      "[Train] Step: 12500, loss: 0.45137, acc: 0.90000\n",
      "[Train] Step: 13000, loss: 0.56679, acc: 0.85000\n",
      "[Train] Step: 13500, loss: 0.75511, acc: 0.85000\n",
      "[Train] Step: 14000, loss: 0.89347, acc: 0.65000\n",
      "[Train] Step: 14500, loss: 0.34916, acc: 0.90000\n",
      "[Train] Step: 15000, loss: 0.66362, acc: 0.70000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Step: 15000, acc: 0.74200\n",
      "[Train] Step: 15500, loss: 0.22467, acc: 0.95000\n",
      "[Train] Step: 16000, loss: 0.58871, acc: 0.70000\n",
      "[Train] Step: 16500, loss: 0.59290, acc: 0.80000\n",
      "[Train] Step: 17000, loss: 0.68080, acc: 0.80000\n",
      "[Train] Step: 17500, loss: 0.43179, acc: 0.90000\n",
      "[Train] Step: 18000, loss: 0.64409, acc: 0.80000\n",
      "[Train] Step: 18500, loss: 0.50077, acc: 0.80000\n",
      "[Train] Step: 19000, loss: 0.57923, acc: 0.75000\n",
      "[Train] Step: 19500, loss: 0.59142, acc: 0.80000\n",
      "[Train] Step: 20000, loss: 0.45756, acc: 0.90000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Step: 20000, acc: 0.78100\n",
      "[Train] Step: 20500, loss: 0.49458, acc: 0.85000\n",
      "[Train] Step: 21000, loss: 0.29568, acc: 0.80000\n",
      "[Train] Step: 21500, loss: 0.95262, acc: 0.70000\n",
      "[Train] Step: 22000, loss: 0.77996, acc: 0.80000\n",
      "[Train] Step: 22500, loss: 0.91134, acc: 0.60000\n",
      "[Train] Step: 23000, loss: 0.77742, acc: 0.80000\n",
      "[Train] Step: 23500, loss: 0.66499, acc: 0.90000\n",
      "[Train] Step: 24000, loss: 0.39414, acc: 0.85000\n",
      "[Train] Step: 24500, loss: 0.63738, acc: 0.75000\n",
      "[Train] Step: 25000, loss: 0.70892, acc: 0.75000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Step: 25000, acc: 0.79150\n",
      "[Train] Step: 25500, loss: 0.41646, acc: 0.90000\n",
      "[Train] Step: 26000, loss: 0.57476, acc: 0.75000\n",
      "[Train] Step: 26500, loss: 0.22949, acc: 0.90000\n",
      "[Train] Step: 27000, loss: 0.39015, acc: 0.85000\n",
      "[Train] Step: 27500, loss: 0.77048, acc: 0.65000\n",
      "[Train] Step: 28000, loss: 0.31276, acc: 0.85000\n",
      "[Train] Step: 28500, loss: 0.62186, acc: 0.75000\n",
      "[Train] Step: 29000, loss: 0.64069, acc: 0.85000\n",
      "[Train] Step: 29500, loss: 0.56906, acc: 0.80000\n",
      "[Train] Step: 30000, loss: 0.64487, acc: 0.80000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Step: 30000, acc: 0.80550\n",
      "[Train] Step: 30500, loss: 0.73105, acc: 0.80000\n",
      "[Train] Step: 31000, loss: 0.49018, acc: 0.85000\n",
      "[Train] Step: 31500, loss: 0.32124, acc: 0.90000\n",
      "[Train] Step: 32000, loss: 0.71318, acc: 0.60000\n",
      "[Train] Step: 32500, loss: 0.51012, acc: 0.75000\n",
      "[Train] Step: 33000, loss: 0.23586, acc: 0.95000\n",
      "[Train] Step: 33500, loss: 0.62851, acc: 0.85000\n",
      "[Train] Step: 34000, loss: 0.39834, acc: 0.85000\n",
      "[Train] Step: 34500, loss: 0.29521, acc: 0.90000\n",
      "[Train] Step: 35000, loss: 0.30675, acc: 0.90000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Step: 35000, acc: 0.80150\n",
      "[Train] Step: 35500, loss: 0.35859, acc: 0.85000\n",
      "[Train] Step: 36000, loss: 0.27603, acc: 0.95000\n",
      "[Train] Step: 36500, loss: 0.78407, acc: 0.75000\n",
      "[Train] Step: 37000, loss: 0.26018, acc: 0.95000\n",
      "[Train] Step: 37500, loss: 0.64886, acc: 0.70000\n",
      "[Train] Step: 38000, loss: 0.19163, acc: 0.95000\n",
      "[Train] Step: 38500, loss: 0.31050, acc: 0.90000\n",
      "[Train] Step: 39000, loss: 0.16882, acc: 0.95000\n",
      "[Train] Step: 39500, loss: 0.68088, acc: 0.80000\n",
      "[Train] Step: 40000, loss: 0.14024, acc: 0.95000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Step: 40000, acc: 0.81650\n",
      "[Train] Step: 40500, loss: 0.38615, acc: 0.85000\n",
      "[Train] Step: 41000, loss: 0.46618, acc: 0.85000\n",
      "[Train] Step: 41500, loss: 0.12940, acc: 0.95000\n",
      "[Train] Step: 42000, loss: 0.59361, acc: 0.80000\n",
      "[Train] Step: 42500, loss: 0.83889, acc: 0.70000\n",
      "[Train] Step: 43000, loss: 0.40613, acc: 0.80000\n",
      "[Train] Step: 43500, loss: 0.86704, acc: 0.75000\n",
      "[Train] Step: 44000, loss: 0.89903, acc: 0.75000\n",
      "[Train] Step: 44500, loss: 0.56207, acc: 0.85000\n",
      "[Train] Step: 45000, loss: 0.26218, acc: 0.85000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Step: 45000, acc: 0.79100\n",
      "[Train] Step: 45500, loss: 0.51446, acc: 0.85000\n",
      "[Train] Step: 46000, loss: 0.61811, acc: 0.70000\n",
      "[Train] Step: 46500, loss: 0.27205, acc: 0.95000\n",
      "[Train] Step: 47000, loss: 0.40497, acc: 0.90000\n",
      "[Train] Step: 47500, loss: 0.30938, acc: 0.85000\n",
      "[Train] Step: 48000, loss: 0.12074, acc: 0.95000\n",
      "[Train] Step: 48500, loss: 0.23381, acc: 0.95000\n",
      "[Train] Step: 49000, loss: 0.22322, acc: 0.95000\n",
      "[Train] Step: 49500, loss: 0.09307, acc: 0.95000\n",
      "[Train] Step: 50000, loss: 0.23561, acc: 0.90000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Step: 50000, acc: 0.80700\n",
      "[Train] Step: 50500, loss: 0.34608, acc: 0.85000\n",
      "[Train] Step: 51000, loss: 0.84159, acc: 0.75000\n",
      "[Train] Step: 51500, loss: 0.45119, acc: 0.85000\n",
      "[Train] Step: 52000, loss: 0.28866, acc: 0.90000\n",
      "[Train] Step: 52500, loss: 0.30102, acc: 0.90000\n",
      "[Train] Step: 53000, loss: 0.16650, acc: 0.95000\n",
      "[Train] Step: 53500, loss: 0.39808, acc: 0.90000\n",
      "[Train] Step: 54000, loss: 0.30985, acc: 0.90000\n",
      "[Train] Step: 54500, loss: 0.41961, acc: 0.80000\n",
      "[Train] Step: 55000, loss: 0.36788, acc: 0.90000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Step: 55000, acc: 0.79900\n",
      "[Train] Step: 55500, loss: 0.08574, acc: 1.00000\n",
      "[Train] Step: 56000, loss: 0.29673, acc: 0.85000\n",
      "[Train] Step: 56500, loss: 0.16490, acc: 0.90000\n",
      "[Train] Step: 57000, loss: 0.33485, acc: 0.80000\n",
      "[Train] Step: 57500, loss: 0.12371, acc: 0.95000\n",
      "[Train] Step: 58000, loss: 0.66707, acc: 0.75000\n",
      "[Train] Step: 58500, loss: 0.20142, acc: 0.95000\n",
      "[Train] Step: 59000, loss: 0.67488, acc: 0.80000\n",
      "[Train] Step: 59500, loss: 0.58560, acc: 0.75000\n",
      "[Train] Step: 60000, loss: 0.21548, acc: 0.90000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Step: 60000, acc: 0.81300\n",
      "[Train] Step: 60500, loss: 0.40787, acc: 0.90000\n",
      "[Train] Step: 61000, loss: 0.34302, acc: 0.85000\n",
      "[Train] Step: 61500, loss: 0.24995, acc: 0.95000\n",
      "[Train] Step: 62000, loss: 0.22006, acc: 0.95000\n",
      "[Train] Step: 62500, loss: 0.60330, acc: 0.75000\n",
      "[Train] Step: 63000, loss: 0.09082, acc: 1.00000\n",
      "[Train] Step: 63500, loss: 0.37265, acc: 0.85000\n",
      "[Train] Step: 64000, loss: 0.34242, acc: 0.95000\n",
      "[Train] Step: 64500, loss: 0.34353, acc: 0.85000\n",
      "[Train] Step: 65000, loss: 0.29607, acc: 0.90000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Step: 65000, acc: 0.81050\n",
      "[Train] Step: 65500, loss: 0.35295, acc: 0.85000\n",
      "[Train] Step: 66000, loss: 0.34059, acc: 0.90000\n",
      "[Train] Step: 66500, loss: 0.14837, acc: 1.00000\n",
      "[Train] Step: 67000, loss: 0.81194, acc: 0.80000\n",
      "[Train] Step: 67500, loss: 0.12743, acc: 0.95000\n",
      "[Train] Step: 68000, loss: 0.25175, acc: 0.95000\n",
      "[Train] Step: 68500, loss: 0.65500, acc: 0.75000\n",
      "[Train] Step: 69000, loss: 0.68219, acc: 0.75000\n",
      "[Train] Step: 69500, loss: 0.29849, acc: 0.80000\n",
      "[Train] Step: 70000, loss: 0.10814, acc: 0.95000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Step: 70000, acc: 0.80450\n",
      "[Train] Step: 70500, loss: 0.42625, acc: 0.75000\n",
      "[Train] Step: 71000, loss: 0.28527, acc: 0.90000\n",
      "[Train] Step: 71500, loss: 0.66370, acc: 0.80000\n",
      "[Train] Step: 72000, loss: 0.52774, acc: 0.85000\n",
      "[Train] Step: 72500, loss: 0.41620, acc: 0.80000\n",
      "[Train] Step: 73000, loss: 0.50363, acc: 0.85000\n",
      "[Train] Step: 73500, loss: 0.14506, acc: 0.95000\n",
      "[Train] Step: 74000, loss: 0.15594, acc: 0.95000\n",
      "[Train] Step: 74500, loss: 0.19119, acc: 0.85000\n",
      "[Train] Step: 75000, loss: 0.24454, acc: 0.90000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Step: 75000, acc: 0.81400\n",
      "[Train] Step: 75500, loss: 0.09665, acc: 0.95000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Step: 76000, loss: 0.21699, acc: 0.90000\n",
      "[Train] Step: 76500, loss: 0.25512, acc: 0.95000\n",
      "[Train] Step: 77000, loss: 0.71854, acc: 0.75000\n",
      "[Train] Step: 77500, loss: 0.37641, acc: 0.85000\n",
      "[Train] Step: 78000, loss: 0.16266, acc: 0.95000\n",
      "[Train] Step: 78500, loss: 0.31521, acc: 0.90000\n",
      "[Train] Step: 79000, loss: 0.13569, acc: 0.95000\n",
      "[Train] Step: 79500, loss: 0.38104, acc: 0.80000\n",
      "[Train] Step: 80000, loss: 0.07283, acc: 1.00000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Step: 80000, acc: 0.81900\n",
      "[Train] Step: 80500, loss: 0.19052, acc: 0.95000\n",
      "[Train] Step: 81000, loss: 0.45169, acc: 0.85000\n",
      "[Train] Step: 81500, loss: 0.28069, acc: 0.90000\n",
      "[Train] Step: 82000, loss: 0.58423, acc: 0.80000\n",
      "[Train] Step: 82500, loss: 0.19257, acc: 0.95000\n",
      "[Train] Step: 83000, loss: 0.24692, acc: 0.90000\n",
      "[Train] Step: 83500, loss: 0.39929, acc: 0.85000\n",
      "[Train] Step: 84000, loss: 0.52644, acc: 0.75000\n",
      "[Train] Step: 84500, loss: 0.34544, acc: 0.95000\n",
      "[Train] Step: 85000, loss: 0.03198, acc: 1.00000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Step: 85000, acc: 0.81250\n",
      "[Train] Step: 85500, loss: 0.19996, acc: 0.95000\n",
      "[Train] Step: 86000, loss: 0.13828, acc: 0.95000\n",
      "[Train] Step: 86500, loss: 0.19260, acc: 0.85000\n",
      "[Train] Step: 87000, loss: 0.25522, acc: 0.85000\n",
      "[Train] Step: 87500, loss: 0.30255, acc: 0.95000\n",
      "[Train] Step: 88000, loss: 0.26048, acc: 0.85000\n",
      "[Train] Step: 88500, loss: 0.57725, acc: 0.80000\n",
      "[Train] Step: 89000, loss: 0.61615, acc: 0.75000\n",
      "[Train] Step: 89500, loss: 0.38735, acc: 0.85000\n",
      "[Train] Step: 90000, loss: 0.10132, acc: 0.95000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Step: 90000, acc: 0.81650\n",
      "[Train] Step: 90500, loss: 0.22142, acc: 0.90000\n",
      "[Train] Step: 91000, loss: 0.22574, acc: 0.85000\n",
      "[Train] Step: 91500, loss: 0.69238, acc: 0.75000\n",
      "[Train] Step: 92000, loss: 0.39224, acc: 0.80000\n",
      "[Train] Step: 92500, loss: 0.12354, acc: 1.00000\n",
      "[Train] Step: 93000, loss: 0.13330, acc: 0.95000\n",
      "[Train] Step: 93500, loss: 0.27310, acc: 0.90000\n",
      "[Train] Step: 94000, loss: 0.42004, acc: 0.95000\n",
      "[Train] Step: 94500, loss: 0.24286, acc: 0.95000\n",
      "[Train] Step: 95000, loss: 0.27826, acc: 0.90000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Step: 95000, acc: 0.81100\n",
      "[Train] Step: 95500, loss: 0.26632, acc: 0.90000\n",
      "[Train] Step: 96000, loss: 0.29483, acc: 0.90000\n",
      "[Train] Step: 96500, loss: 0.20108, acc: 0.95000\n",
      "[Train] Step: 97000, loss: 0.08332, acc: 0.95000\n",
      "[Train] Step: 97500, loss: 0.17825, acc: 0.95000\n",
      "[Train] Step: 98000, loss: 0.14503, acc: 0.95000\n",
      "[Train] Step: 98500, loss: 0.20466, acc: 0.95000\n",
      "[Train] Step: 99000, loss: 0.20248, acc: 0.95000\n",
      "[Train] Step: 99500, loss: 0.54471, acc: 0.75000\n",
      "[Train] Step: 100000, loss: 0.35054, acc: 0.85000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Step: 100000, acc: 0.83450\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "train_steps = 100000\n",
    "test_steps = 100\n",
    "\n",
    "output_summary_every_steps = 100\n",
    "output_model_every_stpes = 100\n",
    "\n",
    "# train 100k: 78.05% -> 82.6% -> 83.4% -> 85.6%\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    train_writer = tf.summary.FileWriter(train_log_dir, sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(test_log_dir)\n",
    "    \n",
    "    fixed_test_batch_data, fixed_test_batch_labels \\\n",
    "        = test_data.next_batch(batch_size)\n",
    "    \n",
    "#     if os.path.exists(model_path + '.index'):\n",
    "#         saver.restore(sess, model_path)\n",
    "#         print('model restored from %s' % model_path)\n",
    "#     else:\n",
    "#         print('model %s does not exist' % model_path)\n",
    "\n",
    "    for i in range(train_steps):\n",
    "        batch_data,batch_labels = train_data.next_batch(batch_size)\n",
    "        eval_ops = [loss, accuracy, train_op]\n",
    "        should_output_summary = ((i+1) % output_summary_every_steps == 0)\n",
    "        if should_output_summary:\n",
    "            eval_ops.append(merged_summary)\n",
    "        \n",
    "        eval_ops_results = sess.run(\n",
    "            eval_ops,\n",
    "            feed_dict = {\n",
    "                x: batch_data, \n",
    "                y: batch_labels,\n",
    "                is_training: True,})\n",
    "        loss_val, acc_val = eval_ops_results[0:2]\n",
    "        if should_output_summary:\n",
    "            train_summary_str = eval_ops_results[-1]\n",
    "            train_writer.add_summary(train_summary_str, i+1)\n",
    "            test_summary_str = sess.run([merged_summary_test],\n",
    "                                        feed_dict={\n",
    "                                            x: fixed_test_batch_data,\n",
    "                                            y: fixed_test_batch_labels,\n",
    "                                            is_training: False,\n",
    "                                        })[0]\n",
    "            test_writer.add_summary(test_summary_str, i+1)\n",
    "            \n",
    "        if (i+1) % 500 == 0:\n",
    "            print('[Train] Step: %d, loss: %4.5f, acc: %4.5f' \\\n",
    "                 % (i+1, loss_val, acc_val))\n",
    "        if (i+1) % 5000 == 0:\n",
    "            test_data = CifarData(test_filenames,False)\n",
    "            all_test_acc_val = []\n",
    "            for j in range(test_steps):\n",
    "                test_batch_data, test_batch_labels \\\n",
    "                    = test_data.next_batch(batch_size)\n",
    "                test_acc_val = sess.run(\n",
    "                    [accuracy],\n",
    "                    feed_dict = {\n",
    "                        x: test_batch_data,\n",
    "                        y: test_batch_labels,\n",
    "                        is_training: False,})\n",
    "                all_test_acc_val.append(test_acc_val)\n",
    "            test_acc = np.mean(all_test_acc_val)\n",
    "            print('[Test] Step: %d, acc: %4.5f' % (i+1,test_acc))\n",
    "#         if (i+1) % output_model_every_stpes == 0:\n",
    "#             saver.save(sess,\n",
    "#                        os.path.join(model_dir, 'ckp-%05d' % (i+1)))\n",
    "#             print('model saved to ckp-%05d' % (i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 10K -> 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
